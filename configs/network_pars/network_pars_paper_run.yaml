# directory that all other paths are relative to
input_dir: /Users/JoeyA/Research/SPHEREx/spherex_emu/
# file paths are relative w.r.t. the repository directory
# Where to save the network
save_dir: ./emulators/stacked_transformer_paper_run/
# Where your training set lives
training_dir: ../../Data/SPHEREx-Data/training_set_eft_new/
# Location of config file with cosmology + bias parameters
# This file contains all of the parameter ranges
cosmo_dir : ./configs/cosmo_pars/cosmo_pars_eft_new.yaml

model_type : stacked_transformer
loss_type : hyperbolic_chi2

num_cosmo_params    : 5
num_nuisance_params : 3
num_tracers : 2
num_zbins   : 2
num_ells    : 2
num_kbins   : 25

# specifications are for each network - will be repeated for each sample / redshift bin
galaxy_ps_emulator:
  # mlp parameters
  num_mlp_blocks      : 2
  num_block_layers    : 5
  use_skip_connection : True

  # transformer parameters
  num_transformer_blocks : 2
  split_dim              : 5
  split_size             : 20

ps_nw_emulator:
  num_kbins : 256
  # mlp parameters
  num_mlp_blocks      : 2
  num_block_layers    : 3
  use_skip_connection : True

  # transformer parameters
  num_transformer_blocks : 1
  split_dim              : 8
  split_size             : 64

# Training parameters
num_epochs: 400
learning_rate: 0.005
batch_size: 250
training_set_fraction : 1.
early_stopping_epochs: 25
weight_initialization: He
optimizer_type : Adam

# whether to re-calculate the training-set loss at the end of each epoch
# Setting to true gives more accurate loss stats, but is slower
# Validation set loss is not changed by this option
recalculate_train_loss : False
# whether to attempt training on GPU
# NOTE: Small networks will possibly train faster on CPU!
use_gpu : True
# whether to print progress during training
print_progress : True
