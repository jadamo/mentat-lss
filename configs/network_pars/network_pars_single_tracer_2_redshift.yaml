
model_type : MLP_single_tracer_multi_redshift

num_cosmo_params : 4
cosmo_params : 
  h       : [0.4, 1.0]
  omega_c : [0.05, 0.3]
  As      : [1.2e-9,2.7e-9]
  fnl     : [-50, 50]
num_bias_params : 1 # The number of nuisance parameters per tracer and redshift bin
bias_params : 
  b1   : [1., 4.]

output_kbins : 25

num_tracers : 1
num_zbins   : 2

num_mlp_blocks      : 2
mlp_dims            : [20, 40, 50]
num_block_layers    : 4
use_batchnorm       : True
use_skip_connection : True

# Training parameters
num_epochs: 350
learning_rate: [4.65e-04, 1.0e-4]
batch_size: 100
training_set_fraction : 1.0
early_stopping_epochs: 25
weight_initialization: He
optimizer_type : Adam
use_gpu : True

# Where to save the network
save_dir: /home/joeadamo/Research/SPHEREx/spherex_emu/emulators/mlp_multi_tracer/
# Where your training set lives
training_dir: /home/joeadamo/Research/Data/SPHEREx-Data/Training-Set-EFT-1t-1z/