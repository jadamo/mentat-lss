# Where to save the network
save_dir: /home/joeadamo/Research/SPHEREx/spherex_emu/emulators/mlp_multi_sample/
# Where your training set lives
training_dir: /home/joeadamo/Research/Data/SPHEREx-Data/Training-Set-EFT-2s-2z/

model_type : MLP_multi_sample_multi_redshift

output_kbins : 25
num_samples : 2
num_zbins   : 2

num_mlp_blocks      : 2
mlp_dims            : [20, 40, 50]
num_block_layers    : 4
use_batchnorm       : True
use_skip_connection : True

# Training parameters
num_epochs: 350
learning_rate: [4.65e-04, 1.0e-4]
batch_size: 100
training_set_fraction : 1.0
early_stopping_epochs: 25
weight_initialization: He
optimizer_type : Adam
use_gpu : False